{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sequence = 20\n",
    "after_sequence = 5\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "custom dataset for illegal dumping project\n",
    "background version\n",
    "'''\n",
    "\n",
    "'''\n",
    "map-style dataset\n",
    "'''\n",
    "\n",
    "class Illegal_dumping_dataset(Dataset):\n",
    "    def __init__(self, root, normalize=False):\n",
    "        super(Illegal_dumping_dataset, self).__init__()\n",
    "        self.root = root # path to the datasets e.g) train, val, and test\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        if normalize:\n",
    "            # get the mean/std values along the channel dimension\n",
    "            mean = data.mean(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            std = data.std(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            data = (data - mean) / std\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "\n",
    "        # this has all the names of clips as a list\n",
    "        self.clips = glob.glob(f\"{root}/*.avi\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # legnth of the dataset is the number of elements in self.clips\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get ith clip in a tensor\n",
    "        frames, _, _ = torchvision.io.read_video(self.clips[index], pts_unit = \"sec\", output_format = \"TCHW\")\n",
    "\n",
    "        # do normalization\n",
    "        frames = frames / 255.0\n",
    "\n",
    "        # frame_batch contains 10 frames that the model watches\n",
    "        # target_batch contains 1 frame that the model predicts based on the previous 10 frames\n",
    "\n",
    "        # get frames numbers to be sampled\n",
    "        sample_frame_nums = torch.linspace(0, len(frames)-1, pre_sequence + after_sequence, dtype=int)\n",
    "\n",
    "        # get frames accordingly\n",
    "        sampled_frames = [frames[x] for x in sample_frame_nums]\n",
    "\n",
    "        # stack those frames\n",
    "        stacked_sampled_frames = torch.stack(sampled_frames)\n",
    "\n",
    "        return stacked_sampled_frames[:pre_sequence], stacked_sampled_frames[pre_sequence:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "iterable dataset_version 1\n",
    "\n",
    "'''\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class Illegal_dumping_dataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, root, normalize=False):\n",
    "        super(Illegal_dumping_dataset, self).__init__()\n",
    "        self.root = root # path to the datasets e.g) train, val, and test\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        if normalize:\n",
    "            # get the mean/std values along the channel dimension\n",
    "            mean = data.mean(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            std = data.std(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            data = (data - mean) / std\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "\n",
    "        # this has all the names of clips as a list\n",
    "        self.clips = glob.glob(f\"{root}/*.avi\")\n",
    "\n",
    "        self.length = len(self.clips)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        # iter_csv = pd.read_csv(self.data_path, sep='\\t', iterator=True, chunksize=1)\n",
    "        # for line in iter_csv:\n",
    "        #     line = line['text'].item()\n",
    "        #     yield line\n",
    "\n",
    "        for clip in self.clips:\n",
    "            # get ith clip in a tensor\n",
    "\n",
    "            frames, _, _ = torchvision.io.read_video(clip, pts_unit = \"sec\", output_format = \"TCHW\")\n",
    "            sample_frame_nums = torch.linspace(0, len(frames)-1, pre_sequence + after_sequence, dtype=int)\n",
    "\n",
    "            sampled_frames = [frames[x] for x in sample_frame_nums]\n",
    "\n",
    "            del frames\n",
    "\n",
    "            stacked_sampled_frames = torch.stack(sampled_frames)\n",
    "            \n",
    "            yield stacked_sampled_frames[:pre_sequence], stacked_sampled_frames[pre_sequence:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "iterable dataset_version 2; using opencv\n",
    "\n",
    "'''\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class Illegal_dumping_dataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, root, pre_seq, after_seq, normalize=False):\n",
    "        super(Illegal_dumping_dataset, self).__init__()\n",
    "        self.root = root # path to the datasets e.g) train, val, and test\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "        if normalize:\n",
    "            # get the mean/std values along the channel dimension\n",
    "            mean = data.mean(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            std = data.std(axis=(0, 1, 2, 3)).reshape(1, 1, -1, 1, 1)\n",
    "            data = (data - mean) / std\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "\n",
    "        # this has all the names of clips as a list\n",
    "        self.clips = glob.glob(f\"{root}/*.avi\")\n",
    "\n",
    "        self.length = len(self.clips)\n",
    "\n",
    "        self.pre_seq = pre_seq\n",
    "        self.after_seq = after_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for clip in self.clips:\n",
    "\n",
    "            cap = cv2.VideoCapture(clip)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "            sample_frame_nums = np.linspace(0, total_frames-1, num = self.pre_seq + self.after_seq, dtype= int)\n",
    "\n",
    "            sampled_frames = []\n",
    "\n",
    "            for frame_num in sample_frame_nums:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if ret:\n",
    "                    sampled_frames.append(frame.transpose(2, 0, 1))\n",
    "            \n",
    "            #cap.release()\n",
    "\n",
    "            result = np.stack(sampled_frames)\n",
    "\n",
    "            dataset = torch.tensor(result)\n",
    "\n",
    "            yield dataset[:pre_sequence] / 255, dataset[pre_sequence:] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vis-ms/OpenSTL/custom_bg_many\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = Illegal_dumping_dataset(root = \"./train\")\n",
    "# val_set = Illegal_dumping_dataset(root = \"./val\")\n",
    "# test_set = Illegal_dumping_dataset(root = \"./test\")\n",
    "\n",
    "train_set = Illegal_dumping_dataset(\"./train\", pre_sequence, after_sequence)\n",
    "val_set = Illegal_dumping_dataset( \"./val\", pre_sequence, after_sequence)\n",
    "test_set = Illegal_dumping_dataset( \"./test\", pre_sequence, after_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader_train = torch.utils.data.DataLoader(\n",
    "#     train_set, batch_size = bs, shuffle=True, pin_memory=True)\n",
    "# dataloader_val = torch.utils.data.DataLoader(\n",
    "#     val_set, batch_size = bs, shuffle=True, pin_memory=True)\n",
    "# dataloader_test = torch.utils.data.DataLoader(\n",
    "#     test_set, batch_size = bs, shuffle=True, pin_memory=True)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size = bs)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size = bs)\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size = bs)\n",
    "\n",
    "# dataloader_train = torch.utils.data.DataLoader(\n",
    "#     train_set, batch_size = bs, shuffle = True)\n",
    "# dataloader_val = torch.utils.data.DataLoader(\n",
    "#     val_set, batch_size = bs,  shuffle = True,)\n",
    "# dataloader_test = torch.utils.data.DataLoader(\n",
    "#     test_set, batch_size = bs, shuffle = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "count = 1\n",
    "for t in dataloader_train:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for t in dataloader_train:\n",
    "    temp = t\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20, 3, 360, 640])\n"
     ]
    }
   ],
   "source": [
    "print(t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3, 360, 640])\n"
     ]
    }
   ],
   "source": [
    "print(t[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print(getsizeof(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_training_config = {\n",
    "    'pre_seq_length': pre_sequence,\n",
    "    'aft_seq_length': after_sequence,\n",
    "    'total_length': pre_sequence + after_sequence,\n",
    "    'batch_size': bs,\n",
    "    'val_batch_size': bs,\n",
    "    'epoch': 1000,\n",
    "    'lr': 0.001,   \n",
    "    'metrics': ['mse', 'mae', \"psnr\", \"ssim\"],\n",
    "\n",
    "    'ex_name': 'custom_exp',\n",
    "    'dataname': 'custom',\n",
    "    'in_shape': [pre_sequence, 3, 360, 640]\n",
    "}\n",
    "\n",
    "custom_model_config = {\n",
    "    # For MetaVP models, the most important hyperparameters are: \n",
    "    # N_S, N_T, hid_S, hid_T, model_type\n",
    "    #'method': 'SimVP',\n",
    "    # Users can either using a config file or directly set these hyperparameters \n",
    "    'config_file': 'configs/custom/example_model.py',\n",
    "    \n",
    "    # Here, we directly set these parameters\n",
    "    'model_type': 'IncepU',\n",
    "    'N_S': 4,\n",
    "    'N_T': 8,\n",
    "    'hid_S': 64,\n",
    "    'hid_T': 256,\n",
    "    'sched': 'onecycle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use non-distributed mode with GPU: cuda:0\n",
      "Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\n",
      "CUDA available: True\n",
      "CUDA_HOME: /usr/local/cuda-11.8\n",
      "NVCC: Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "PyTorch: 2.0.1+cu117\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.9.3  (built against CUDA 11.8)\n",
      "    - Built with CuDNN 8.5\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.15.2+cu117\n",
      "OpenCV: 4.8.0\n",
      "openstl: 0.3.0\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "device: \tcuda\t\n",
      "dist: \tFalse\t\n",
      "display_step: \t10\t\n",
      "res_dir: \twork_dirs\t\n",
      "ex_name: \tcustom_exp\t\n",
      "use_gpu: \tTrue\t\n",
      "fp16: \tFalse\t\n",
      "torchscript: \tFalse\t\n",
      "seed: \t42\t\n",
      "diff_seed: \tFalse\t\n",
      "fps: \tFalse\t\n",
      "empty_cache: \tTrue\t\n",
      "find_unused_parameters: \tFalse\t\n",
      "broadcast_buffers: \tTrue\t\n",
      "resume_from: \tNone\t\n",
      "auto_resume: \tFalse\t\n",
      "test: \tFalse\t\n",
      "deterministic: \tFalse\t\n",
      "launcher: \tnone\t\n",
      "local_rank: \t0\t\n",
      "port: \t29500\t\n",
      "batch_size: \t1\t\n",
      "val_batch_size: \t1\t\n",
      "num_workers: \t4\t\n",
      "data_root: \t./data\t\n",
      "dataname: \tcustom\t\n",
      "pre_seq_length: \t1\t\n",
      "aft_seq_length: \t1\t\n",
      "total_length: \t2\t\n",
      "use_augment: \tFalse\t\n",
      "use_prefetcher: \tFalse\t\n",
      "drop_last: \tFalse\t\n",
      "method: \tsimvp\t\n",
      "config_file: \tconfigs/custom/example_model.py\t\n",
      "model_type: \tIncepU\t\n",
      "drop: \t0.0\t\n",
      "drop_path: \t0.0\t\n",
      "overwrite: \tFalse\t\n",
      "epoch: \t1000\t\n",
      "log_step: \t1\t\n",
      "opt: \tadam\t\n",
      "opt_eps: \tNone\t\n",
      "opt_betas: \tNone\t\n",
      "momentum: \t0.9\t\n",
      "weight_decay: \t0.0\t\n",
      "clip_grad: \tNone\t\n",
      "clip_mode: \tnorm\t\n",
      "early_stop_epoch: \t-1\t\n",
      "sched: \tonecycle\t\n",
      "lr: \t0.001\t\n",
      "lr_k_decay: \t1.0\t\n",
      "warmup_lr: \t1e-05\t\n",
      "min_lr: \t1e-06\t\n",
      "final_div_factor: \t10000.0\t\n",
      "warmup_epoch: \t0\t\n",
      "decay_epoch: \t100\t\n",
      "decay_rate: \t0.1\t\n",
      "filter_bias_and_bn: \tFalse\t\n",
      "metrics: \t['mse', 'mae', 'psnr', 'ssim']\t\n",
      "in_shape: \t[1, 3, 360, 640]\t\n",
      "N_S: \t4\t\n",
      "N_T: \t8\t\n",
      "hid_S: \t64\t\n",
      "hid_T: \t256\t\n",
      "Model info:\n",
      "SimVP_Model(\n",
      "  (enc): Encoder(\n",
      "    (enc): Sequential(\n",
      "      (0): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (1): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (2): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (3): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec): Decoder(\n",
      "    (dec): Sequential(\n",
      "      (0): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): PixelShuffle(upscale_factor=2)\n",
      "          )\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (1): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (2): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): PixelShuffle(upscale_factor=2)\n",
      "          )\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (3): ConvSC(\n",
      "        (conv): BasicConv2d(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (readout): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (hid): MidIncepNet(\n",
      "    (enc): Sequential(\n",
      "      (0): gInception_ST(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dec): Sequential(\n",
      "      (0): gInception_ST(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): gInception_ST(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (layers): Sequential(\n",
      "          (0): GroupConv2d(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n",
      "            (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (1): GroupConv2d(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n",
      "            (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (2): GroupConv2d(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=8)\n",
      "            (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "          (3): GroupConv2d(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5), groups=8)\n",
      "            (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "            (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| module                 | #parameters or shape   | #flops     |\n",
      "|:-----------------------|:-----------------------|:-----------|\n",
      "| model                  | 14.004M                | 0.222T     |\n",
      "|  enc.enc               |  0.113M                |  5.291G    |\n",
      "|   enc.enc.0.conv       |   1.92K                |   0.472G   |\n",
      "|    enc.enc.0.conv.conv |    1.792K              |    0.398G  |\n",
      "|    enc.enc.0.conv.norm |    0.128K              |    73.728M |\n",
      "|   enc.enc.1.conv       |   37.056K              |   2.142G   |\n",
      "|    enc.enc.1.conv.conv |    36.928K             |    2.123G  |\n",
      "|    enc.enc.1.conv.norm |    0.128K              |    18.432M |\n",
      "|   enc.enc.2.conv       |   37.056K              |   2.142G   |\n",
      "|    enc.enc.2.conv.conv |    36.928K             |    2.123G  |\n",
      "|    enc.enc.2.conv.norm |    0.128K              |    18.432M |\n",
      "|   enc.enc.3.conv       |   37.056K              |   0.535G   |\n",
      "|    enc.enc.3.conv.conv |    36.928K             |    0.531G  |\n",
      "|    enc.enc.3.conv.norm |    0.128K              |    4.608M  |\n",
      "|  dec                   |  0.37M                 |  21.462G   |\n",
      "|   dec.dec              |   0.37M                |   21.418G  |\n",
      "|    dec.dec.0.conv      |    0.148M              |    2.142G  |\n",
      "|    dec.dec.1.conv      |    37.056K             |    2.142G  |\n",
      "|    dec.dec.2.conv      |    0.148M              |    8.567G  |\n",
      "|    dec.dec.3.conv      |    37.056K             |    8.567G  |\n",
      "|   dec.readout          |   0.195K               |   44.237M  |\n",
      "|    dec.readout.weight  |    (3, 64, 1, 1)       |            |\n",
      "|    dec.readout.bias    |    (3,)                |            |\n",
      "|  hid                   |  13.521M               |  0.195T    |\n",
      "|   hid.enc              |   6.948M               |   0.1T     |\n",
      "|    hid.enc.0           |    0.847M              |    12.224G |\n",
      "|    hid.enc.1           |    0.872M              |    12.578G |\n",
      "|    hid.enc.2           |    0.872M              |    12.578G |\n",
      "|    hid.enc.3           |    0.872M              |    12.578G |\n",
      "|    hid.enc.4           |    0.872M              |    12.578G |\n",
      "|    hid.enc.5           |    0.872M              |    12.578G |\n",
      "|    hid.enc.6           |    0.872M              |    12.578G |\n",
      "|    hid.enc.7           |    0.872M              |    12.578G |\n",
      "|   hid.dec              |   6.573M               |   94.847G  |\n",
      "|    hid.dec.0           |    0.872M              |    12.578G |\n",
      "|    hid.dec.1           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.2           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.3           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.4           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.5           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.6           |    0.904M              |    13.05G  |\n",
      "|    hid.dec.7           |    0.275M              |    3.97G   |\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openstl.api import BaseExperiment\n",
    "from openstl.utils import create_parser\n",
    "\n",
    "args = create_parser().parse_args([])\n",
    "config = args.__dict__\n",
    "\n",
    "# update the training config\n",
    "config.update(custom_training_config)\n",
    "# update the model config\n",
    "config.update(custom_model_config)\n",
    "\n",
    "exp = BaseExperiment(args, dataloaders=(dataloader_train, dataloader_val, dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> training <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/213 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 23.66 GiB total capacity; 19.97 GiB already allocated; 422.19 MiB free; 20.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m35\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m training \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m35\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m exp\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/OpenSTL/openstl/api/train.py:301\u001b[0m, in \u001b[0;36mBaseExperiment.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dist \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader\u001b[39m.\u001b[39msampler, \u001b[39m'\u001b[39m\u001b[39mset_epoch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m--> 301\u001b[0m num_updates, loss_mean, eta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mtrain_one_epoch(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader,\n\u001b[1;32m    302\u001b[0m                                                           epoch, num_updates, eta)\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m=\u001b[39m epoch\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mlog_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/OpenSTL/openstl/methods/simvp.py:72\u001b[0m, in \u001b[0;36mSimVP.train_one_epoch\u001b[0;34m(self, runner, train_loader, epoch, num_updates, eta, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m runner\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mbefore_train_iter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp_autocast():\n\u001b[0;32m---> 72\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(batch_x)\n\u001b[1;32m     73\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(pred_y, batch_y)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist:\n",
      "File \u001b[0;32m~/OpenSTL/openstl/methods/simvp.py:34\u001b[0m, in \u001b[0;36mSimVP._predict\u001b[0;34m(self, batch_x, batch_y, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(batch_x)\n\u001b[1;32m     33\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39maft_seq_length \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpre_seq_length:\n\u001b[0;32m---> 34\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_x)\n\u001b[1;32m     35\u001b[0m     pred_y \u001b[39m=\u001b[39m pred_y[:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39maft_seq_length]\n\u001b[1;32m     36\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39maft_seq_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpre_seq_length:\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenSTL/openstl/models/simvp_model.py:46\u001b[0m, in \u001b[0;36mSimVP_Model.forward\u001b[0;34m(self, x_raw, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m hid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhid(z)\n\u001b[1;32m     44\u001b[0m hid \u001b[39m=\u001b[39m hid\u001b[39m.\u001b[39mreshape(B\u001b[39m*\u001b[39mT, C_, H_, W_)\n\u001b[0;32m---> 46\u001b[0m Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec(hid, skip)\n\u001b[1;32m     47\u001b[0m Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mreshape(B, T, C, H, W)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenSTL/openstl/models/simvp_model.py:95\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, hid, enc1)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hid, enc1\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m         hid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec[i](hid)\n\u001b[1;32m     96\u001b[0m     Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](hid \u001b[39m+\u001b[39m enc1)\n\u001b[1;32m     97\u001b[0m     Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadout(Y)\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenSTL/openstl/modules/simvp_modules.py:77\u001b[0m, in \u001b[0;36mConvSC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 77\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenSTL/openstl/modules/simvp_modules.py:51\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 51\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_norm:\n\u001b[1;32m     53\u001b[0m         y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(y))\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/ostl/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 564.00 MiB (GPU 0; 23.66 GiB total capacity; 19.97 GiB already allocated; 422.19 MiB free; 20.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print('>'*35 + ' training ' + '<'*35)\n",
    "exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>'*35 + ' testing  ' + '<'*35)\n",
    "exp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openstl.utils import show_video_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the given frames from an example\n",
    "inputs = np.load('./work_dirs/custom_exp/saved/inputs.npy')\n",
    "preds = np.load('./work_dirs/custom_exp/saved/preds.npy')\n",
    "trues = np.load('./work_dirs/custom_exp/saved/trues.npy')\n",
    "\n",
    "example_idx = 6\n",
    "show_video_line(trues[example_idx], ncols=after_sequence, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 5\n",
    "show_video_line(trues[example_idx], ncols=after_sequence, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 5\n",
    "show_video_line(preds[example_idx], ncols=after_sequence, vmax=0.6, cbar=False, out_path=None, format='png', use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstl.utils import show_video_gif_multiple\n",
    "\n",
    "example_idx = 6\n",
    "show_video_gif_multiple(inputs[example_idx], trues[example_idx], preds[example_idx], use_rgb=True, out_path='example.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ostl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
